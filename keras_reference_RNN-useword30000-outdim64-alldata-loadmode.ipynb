{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import os\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re_tag = re.compile(r'<[^>]+>')\n",
    "\n",
    "def rm_tags(text):\n",
    "    return re_tag.sub('', text.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ps(fn,fv=''):\n",
    "    print (fn, fv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "## type => train / test\n",
    "## indir => 1, 2, 4, 5, 6 , 7, 8\n",
    "def read_ref_files(typedir, indir, indirmax=9, readmax=100000):\n",
    "    path = \"data/aclImdb/\"\n",
    "    file_list=[]\n",
    "    labels_list = []\n",
    "    \n",
    "    thisdirpath = '%s/%s' % (typedir, indir)\n",
    "    for f in os.listdir(thisdirpath)[0:readmax]:\n",
    "        file_list+= [thisdirpath+os.sep+f]\n",
    "            \n",
    "    #print('read',thisdirpath, 'files:',len(file_list))\n",
    "       \n",
    "    all_labels = ([indir] * len(file_list)) \n",
    "    \n",
    "    all_texts  = []\n",
    "    \n",
    "    for fi in file_list:\n",
    "        with open(fi,encoding='utf8') as file_input:\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))]\n",
    "    \n",
    "    #print('len-all_labels', len(all_labels), 'len-all_texts', len(all_texts))\n",
    "    return all_labels,all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_label, train_text = [], []\n",
    "for x in range(1, 9):\n",
    "    ##tmp_label, tmp_text = read_ref_files(\"train\", x, readmax=5000)\n",
    "    tmp_label, tmp_text = read_ref_files(\"train\", x)\n",
    "    train_label += tmp_label\n",
    "    train_text += tmp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train_text)\n",
    "#print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_label, test_text = [], []\n",
    "for x in range(1, 9):\n",
    "    ##tmp_label, tmp_text = read_ref_files(\"test\", x, readmax=5000)\n",
    "    tmp_label, tmp_text = read_ref_files(\"test\", x)\n",
    "    test_label += tmp_label\n",
    "    test_text += tmp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=30000)\n",
    "token.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##改成數字list\n",
    "train_text_numlst = token.texts_to_sequences(train_text)\n",
    "test_text_numlst  = token.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##改成數字list長度100\n",
    "train_text_numseq = sequence.pad_sequences(train_text_numlst, maxlen=100)\n",
    "test_text_numseq  = sequence.pad_sequences(test_text_numlst,  maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##1 => [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]\n",
    "##2 => [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]\n",
    "train_label_digitlst = np_utils.to_categorical(train_label)\n",
    "test_label_digitlst = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Embedding(output_dim=32,\n",
    "                    input_dim=30000, \n",
    "                    input_length=100))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(SimpleRNN(units=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=256,\n",
    "                activation='relu' ))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=9,\n",
    "                activation='sigmoid' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           960000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 16)                784       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 967,449\n",
      "Trainable params: 967,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 220694 samples, validate on 55174 samples\n",
      "Epoch 1/10\n",
      " - 138s - loss: 0.1036 - acc: 0.9534 - val_loss: 0.9705 - val_acc: 0.8568\n",
      "Epoch 2/10\n",
      " - 138s - loss: 0.0650 - acc: 0.9658 - val_loss: 1.0716 - val_acc: 0.8634\n",
      "Epoch 3/10\n",
      " - 138s - loss: 0.0606 - acc: 0.9676 - val_loss: 1.1724 - val_acc: 0.8675\n",
      "Epoch 4/10\n",
      " - 138s - loss: 0.0585 - acc: 0.9684 - val_loss: 1.3150 - val_acc: 0.8606\n",
      "Epoch 5/10\n",
      " - 140s - loss: 0.0570 - acc: 0.9688 - val_loss: 1.3040 - val_acc: 0.8620\n",
      "Epoch 6/10\n",
      " - 159s - loss: 0.0559 - acc: 0.9693 - val_loss: 1.3732 - val_acc: 0.8651\n",
      "Epoch 7/10\n",
      " - 211s - loss: 0.0551 - acc: 0.9695 - val_loss: 1.3982 - val_acc: 0.8509\n",
      "Epoch 8/10\n",
      " - 201s - loss: 0.0545 - acc: 0.9696 - val_loss: 1.4924 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      " - 173s - loss: 0.0541 - acc: 0.9697 - val_loss: 1.4926 - val_acc: 0.8640\n",
      "Epoch 10/10\n",
      " - 136s - loss: 0.0537 - acc: 0.9698 - val_loss: 1.5686 - val_acc: 0.8325\n"
     ]
    }
   ],
   "source": [
    "##訓練損失(loss) 準確率(acc) 驗證損失(val_loss) 驗證準確率(val_acc)\n",
    "train_history =model.fit(train_text_numseq, train_label_digitlst, batch_size=100, \n",
    "                         epochs=10,verbose=2,\n",
    "                         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81500/81500 [==============================] - 30s 370us/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_text_numseq, test_label_digitlst, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93929787268960407"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81500/81500 [==============================] - 31s 375us/step\n"
     ]
    }
   ],
   "source": [
    "prediction=model.predict_classes(test_text_numseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, ..., 3, 3, 3]), 81500)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction, len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_view(predlst, datalst, ind):\n",
    "    typedic = {1:'作者', 2:'年分', 3:'題名', 4:'出處', 5:'出版地', 6:'出版者', 7:'卷期', 8:'頁次'}\n",
    "    print('預測:%s' % typedic.get(predlst[ind]))\n",
    "    print('資料:%s' % datalst[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:275868\n"
     ]
    }
   ],
   "source": [
    "print('Train data:%s' % len(train_label_digitlst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測:題名\n",
      "資料:Group building for successful inclusion program\n",
      "index:28667\n",
      "---------------------------\n",
      "預測:題名\n",
      "資料:475-486\n",
      "index:74867\n",
      "---------------------------\n",
      "預測:出處\n",
      "資料:112-115\n",
      "index:75262\n",
      "---------------------------\n",
      "預測:題名\n",
      "資料:X and R charts for skewed populations\n",
      "index:37515\n",
      "---------------------------\n",
      "預測:題名\n",
      "資料:吳振麟\n",
      "index:12214\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "runed = 1\n",
    "runmax = 5\n",
    "while runed <= runmax:\n",
    "    runint = random.randint(0, len(prediction)-1)\n",
    "    prediction_view(prediction, test_text, runint)\n",
    "    runed += 1\n",
    "    print('index:%s' % runint)\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "import h5py\n",
    "with open(\"SaveModel/keras_reference_RNN_dim30000_outdim64_leru.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"SaveModel/keras_reference_RNN_dim30000_outdim64_leru.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##save tokenizer\n",
    "import pickle\n",
    "pickle.dump(token, open('SaveModel/keras_reference_RNN_dim30000_outdim64_leru.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token len 275868\n"
     ]
    }
   ],
   "source": [
    "##load tokenizer\n",
    "import pickle\n",
    "load_token = pickle.load(open('SaveModel/keras_reference_RNN_dim30000_outdim64_leru.npy', 'rb'))\n",
    "print('token len', load_token.document_count)\n",
    "##print(load_token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "token len 275868\n"
     ]
    }
   ],
   "source": [
    "json_file = open('SaveModel/keras_reference_RNN_dim30000_outdim64_leru.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"SaveModel/keras_reference_RNN_dim30000_outdim64_leru.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "print('token len', load_token.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##從module & token 中判斷該資料類型\n",
    "def predict_data(loaded_model, load_token, input_text):\n",
    "    #load_token.fit_on_texts([input_text])\n",
    "    print('token len', load_token.document_count)\n",
    "    input_text_numlst = load_token.texts_to_sequences([input_text])\n",
    "    input_text_seq = sequence.pad_sequences(input_text_numlst, maxlen=100)\n",
    "    load_prediction=loaded_model.predict_classes(input_text_seq)\n",
    "    print('load_prediction', load_prediction)\n",
    "    prediction_view(load_prediction, [input_text], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token len 275868\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "load_prediction [3]\n",
      "預測:題名\n",
      "資料:鍾尚恩\n"
     ]
    }
   ],
   "source": [
    "input_text = '''以雲端運算為基礎建立醫療輔助決策系統-以冠狀動脈心臟病為例'''\n",
    "input_text = '''心率減速率和連續心率減速率在心肌梗死後猝死患者中的變化及其意義'''\n",
    "#input_text = '''鍾振文'''\n",
    "#input_text = '''Competition, compatibility, and standards'''\n",
    "#input_text = '''Greeve, H. R.'''\n",
    "##預測:出處\n",
    "#input_text = '''Journal of Public Administration: Research and Theory'''\n",
    "##預測:卷期\n",
    "#input_text = '''69(January)'''\n",
    "##預測:年分\n",
    "#input_text = '''1997'''\n",
    "##預測:卷期\n",
    "#input_text = '''18(7)'''\n",
    "##預測:title\n",
    "#input_text = '''畢祿溪試驗集水區的降水及溪水化學'''\n",
    "#input_text = '''中華民國統計年鑑'''\n",
    "#input_text = '''臺北'''\n",
    "##print(input_text)\n",
    "predict_data(loaded_model, load_token, input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
